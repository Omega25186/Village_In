<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning - Interactive Guide</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        :root {
            --primary: #FF6B6B;
            --secondary: #4ECDC4;
            --accent: #FFE66D;
            --dark: #2C3E50;
            --light: #ECF0F1;
            --code-bg: #2C3E50;
            --math-bg: #F8F9FA;
            --card-bg: #FFFFFF;
            --success: #2ECC71;
            --warning: #F39C12;
            --danger: #E74C3C;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-color: #F5F7FA;
            color: #333;
            line-height: 1.6;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        header {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            padding: 2rem 0;
            text-align: center;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        
        h1 {
            font-size: 2.8rem;
            margin-bottom: 0.5rem;
        }
        
        .subtitle {
            font-size: 1.2rem;
            font-weight: 300;
            max-width: 700px;
            margin: 0 auto;
        }
        
        nav {
            background-color: var(--dark);
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .nav-links {
            display: flex;
            list-style: none;
        }
        
        .nav-links li {
            margin-left: 2rem;
        }
        
        .nav-links a {
            color: white;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s;
        }
        
        .nav-links a:hover {
            color: var(--accent);
        }
        
        .section {
            background: white;
            border-radius: 12px;
            padding: 2.5rem;
            margin: 2rem 0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.05);
        }
        
        .section-header {
            display: flex;
            align-items: center;
            margin-bottom: 1.5rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid var(--light);
        }
        
        .section-icon {
            font-size: 2.5rem;
            margin-right: 1rem;
        }
        
        h2 {
            color: var(--dark);
            font-size: 2rem;
        }
        
        h3 {
            color: var(--primary);
            font-size: 1.5rem;
            margin: 1.5rem 0 1rem;
        }
        
        .content-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 1.5rem 0;
        }
        
        .card {
            background: var(--card-bg);
            border-radius: 10px;
            padding: 1.5rem;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
            border-left: 4px solid var(--primary);
        }
        
        .card h4 {
            color: var(--dark);
            margin-bottom: 0.8rem;
            font-size: 1.2rem;
        }
        
        .code-box, .math-box {
            background-color: var(--code-bg);
            color: white;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow: auto;
            max-height: 400px;
            font-family: 'Courier New', monospace;
            white-space: pre;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .math-box {
            background-color: var(--math-bg);
            color: var(--dark);
            border: 1px solid #ddd;
            font-family: 'Times New Roman', serif;
            overflow: auto;
            max-height: 400px;
            padding: 2rem;
        }
        
        .interactive-demo {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 10px;
            padding: 2rem;
            margin: 2rem 0;
            border: 1px solid #dee2e6;
        }
        
        .demo-controls {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            margin: 1.5rem 0;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            min-width: 200px;
        }
        
        .control-group label {
            font-weight: 600;
            color: var(--dark);
        }
        
        input, select, button, textarea {
            padding: 0.7rem;
            border-radius: 6px;
            border: 1px solid #ccc;
            font-size: 1rem;
        }
        
        button {
            background-color: var(--primary);
            color: white;
            border: none;
            cursor: pointer;
            font-weight: 600;
            transition: background-color 0.3s;
        }
        
        button:hover {
            background-color: #FF5252;
        }
        
        .chart-container {
            position: relative;
            height: 300px;
            margin: 1.5rem 0;
        }
        
        .business-case {
            background-color: #fffbea;
            border-left: 4px solid #f1c40f;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .business-case h4 {
            color: #d35400;
            margin-bottom: 0.5rem;
        }
        
        .highlight {
            background-color: rgba(46, 204, 113, 0.2);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-weight: 600;
        }
        
        .prompt-box {
            background-color: #e8f4f8;
            border-left: 4px solid var(--primary);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .prompt-box h4 {
            color: var(--primary);
            margin-bottom: 0.5rem;
        }
        
        .step {
            background-color: #fffbea;
            border-left: 4px solid #f1c40f;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .step h4 {
            color: #d35400;
            margin-bottom: 0.5rem;
        }
        
        .tab-container {
            margin: 2rem 0;
        }
        
        .tab-buttons {
            display: flex;
            border-bottom: 2px solid var(--light);
        }
        
        .tab-button {
            padding: 1rem 1.5rem;
            background: none;
            border: none;
            cursor: pointer;
            font-weight: 600;
            color: var(--dark);
            transition: all 0.3s;
        }
        
        .tab-button.active {
            color: var(--primary);
            border-bottom: 2px solid var(--primary);
        }
        
        .tab-content {
            display: none;
            padding: 1.5rem 0;
        }
        
        .tab-content.active {
            display: block;
        }
        
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }
        
        .data-table th, .data-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        .data-table th {
            background-color: var(--primary);
            color: white;
        }
        
        .data-table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .message {
            padding: 1rem;
            border-radius: 6px;
            margin: 1rem 0;
        }
        
        .message.success {
            background-color: rgba(46, 204, 113, 0.2);
            border-left: 4px solid var(--success);
        }
        
        .message.error {
            background-color: rgba(231, 76, 60, 0.2);
            border-left: 4px solid var(--danger);
        }
        
        .message.info {
            background-color: rgba(52, 152, 219, 0.2);
            border-left: 4px solid var(--primary);
        }
        
        .neural-network {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 2rem 0;
            padding: 1rem;
            background-color: #f8f9fa;
            border-radius: 8px;
        }
        
        .layer {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 1rem;
        }
        
        .neuron {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background-color: var(--primary);
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            font-weight: bold;
        }
        
        .connection {
            width: 100px;
            height: 2px;
            background-color: #ccc;
            position: relative;
        }
        
        .connection-weight {
            position: absolute;
            top: -10px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 0.8rem;
            color: var(--dark);
        }
        
        footer {
            background-color: var(--dark);
            color: white;
            text-align: center;
            padding: 2rem 0;
            margin-top: 3rem;
        }
        
        @media (max-width: 900px) {
            .content-grid {
                grid-template-columns: 1fr;
            }
            
            .nav-links {
                flex-direction: column;
                gap: 0.5rem;
            }
            
            .nav-links li {
                margin: 0;
            }
            
            .neural-network {
                flex-direction: column;
            }
            
            .connection {
                width: 2px;
                height: 50px;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>🤖 Phase 3: Deep Learning</h1>
            <p class="subtitle">Master the foundations of neural networks and deep learning frameworks through interactive examples and practical applications</p>
        </div>
    </header>
    
    <nav>
        <div class="nav-container">
            <div class="logo">Deep Learning</div>
            <ul class="nav-links">
                <li><a href="#neural-networks">Neural Networks</a></li>
                <li><a href="#frameworks">Frameworks</a></li>
                <li><a href="#cnn-rnn">CNNs & RNNs</a></li>
            </ul>
        </div>
    </nav>
    
    <div class="container">
        <!-- Neural Networks Foundations Section -->
        <section id="neural-networks" class="section">
            <div class="section-header">
                <div class="section-icon">🔹</div>
                <h2>Neural Networks Foundations</h2>
            </div>
            
            <div class="subsection">
                <h3>Perceptron</h3>
                <p>The perceptron is the simplest neural network, consisting of a single neuron that takes inputs, applies weights, and produces an output using an activation function.</p>
                
                <div class="math-box">
The perceptron can be mathematically represented as:

\[
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
\]

Where:
- \(x_i\) are the input features
- \(w_i\) are the weights
- \(b\) is the bias term
- \(f\) is the activation function (typically a step function)

For a binary classification problem with a step activation function:
\[
f(z) = \begin{cases} 
1 & \text{if } z \geq 0 \\
0 & \text{if } z < 0 
\end{cases}
\]

The perceptron learning rule updates weights based on the error:
\[
w_i(t+1) = w_i(t) + \eta (y - \hat{y}) x_i
\]

Where:
- \(\eta\) is the learning rate
- \(y\) is the true label
- \(\hat{y}\) is the predicted label
                </div>
                
                <div class="interactive-demo">
                    <h4>Interactive Perceptron Demo</h4>
                    <p>Adjust the weights and bias to see how the perceptron classifies points.</p>
                    
                    <div class="demo-controls">
                        <div class="control-group">
                            <label for="weight1">Weight 1:</label>
                            <input type="range" id="weight1" min="-2" max="2" step="0.1" value="1">
                            <span id="weight1-value">1</span>
                        </div>
                        <div class="control-group">
                            <label for="weight2">Weight 2:</label>
                            <input type="range" id="weight2" min="-2" max="2" step="0.1" value="1">
                            <span id="weight2-value">1</span>
                        </div>
                        <div class="control-group">
                            <label for="bias">Bias:</label>
                            <input type="range" id="bias" min="-2" max="2" step="0.1" value="0">
                            <span id="bias-value">0</span>
                        </div>
                        <button id="update-perceptron">Update Perceptron</button>
                    </div>
                    
                    <div class="chart-container">
                        <canvas id="perceptron-chart"></canvas>
                    </div>
                    
                    <div class="code-box">
# Perceptron implementation
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.1, n_iterations=100):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        
    def fit(self, X, y):
        # Initialize weights and bias
        self.weights = np.zeros(X.shape[1])
        self.bias = 0
        
        # Learning loop
        for _ in range(self.n_iterations):
            for i in range(X.shape[0]):
                # Calculate prediction
                linear_output = np.dot(X[i], self.weights) + self.bias
                prediction = 1 if linear_output >= 0 else 0
                
                # Update weights and bias
                update = self.learning_rate * (y[i] - prediction)
                self.weights += update * X[i]
                self.bias += update
                
    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.where(linear_output >= 0, 1, 0)

# Example usage
X = np.array([[2, 3], [1, 1], [4, 5], [5, 4]])
y = np.array([1, 0, 1, 1])

perceptron = Perceptron()
perceptron.fit(X, y)

# Test prediction
test_point = np.array([3, 2])
prediction = perceptron.predict(test_point.reshape(1, -1))
print(f"Prediction for {test_point}: {prediction[0]}")
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>Multi-layer Perceptrons (MLP)</h3>
                <p>Multi-layer Perceptrons extend the perceptron concept by having multiple layers of neurons, allowing the network to learn complex patterns and non-linear relationships.</p>
                
                <div class="math-box">
An MLP consists of:
1. Input layer: Receives the raw data
2. Hidden layers: Process the data through weighted connections
3. Output layer: Produces the final prediction

Mathematical representation for a layer:

\[
\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}
\]
\[
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{z}^{[l]})
\]

Where:
- \(\mathbf{W}^{[l]}\) is the weight matrix for layer \(l\)
- \(\mathbf{b}^{[l]}\) is the bias vector for layer \(l\)
- \(\mathbf{a}^{[l-1]}\) is the activation from the previous layer
- \(g^{[l]}\) is the activation function for layer \(l\)
- \(\mathbf{a}^{[l]}\) is the activation of layer \(l\)

For a network with \(L\) layers, the forward propagation is:
\[
\mathbf{a}^{[0]} = \mathbf{X} \quad \text{(input)}
\]
\[
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}) \quad \text{for } l = 1, 2, ..., L
\]
\[
\hat{\mathbf{y}} = \mathbf{a}^{[L]} \quad \text{(output)}
\]
                </div>
                
                <div class="interactive-demo">
                    <h4>Interactive MLP Visualization</h4>
                    <p>Visualize the structure of a multi-layer perceptron and see how data flows through the network.</p>
                    
                    <div class="demo-controls">
                        <div class="control-group">
                            <label for="input-neurons">Input Neurons:</label>
                            <select id="input-neurons">
                                <option value="2">2</option>
                                <option value="3">3</option>
                                <option value="4">4</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label for="hidden-layers">Hidden Layers:</label>
                            <select id="hidden-layers">
                                <option value="1">1</option>
                                <option value="2">2</option>
                                <option value="3">3</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label for="hidden-neurons">Neurons per Hidden Layer:</label>
                            <select id="hidden-neurons">
                                <option value="3">3</option>
                                <option value="4">4</option>
                                <option value="5">5</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label for="output-neurons">Output Neurons:</label>
                            <select id="output-neurons">
                                <option value="1">1</option>
                                <option value="2">2</option>
                                <option value="3">3</option>
                            </select>
                        </div>
                        <button id="update-mlp">Update MLP</button>
                    </div>
                    
                    <div id="mlp-visualization" class="neural-network"></div>
                    
                    <div class="code-box">
# Multi-layer Perceptron implementation
import numpy as np

class MLP:
    def __init__(self, layer_sizes, activation='relu'):
        self.layer_sizes = layer_sizes
        self.activation = activation
        self.num_layers = len(layer_sizes)
        
        # Initialize weights and biases
        self.weights = []
        self.biases = []
        
        for i in range(self.num_layers - 1):
            # Xavier initialization for weights
            scale = np.sqrt(2.0 / (layer_sizes[i] + layer_sizes[i+1]))
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * scale)
            self.biases.append(np.zeros((1, layer_sizes[i+1])))
    
    def _activation(self, x):
        if self.activation == 'relu':
            return np.maximum(0, x)
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation == 'tanh':
            return np.tanh(x)
    
    def _activation_derivative(self, x):
        if self.activation == 'relu':
            return (x > 0).astype(float)
        elif self.activation == 'sigmoid':
            s = 1 / (1 + np.exp(-x))
            return s * (1 - s)
        elif self.activation == 'tanh':
            return 1 - np.tanh(x) ** 2
    
    def forward(self, X):
        self.activations = [X]
        self.z_values = []
        
        for i in range(self.num_layers - 1):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            
            if i == self.num_layers - 2:  # Output layer
                a = z  # Linear activation for regression
            else:
                a = self._activation(z)
            
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X, y, learning_rate):
        m = X.shape[0]
        
        # Forward pass
        output = self.forward(X)
        
        # Backward pass
        deltas = [output - y]
        
        for i in range(self.num_layers - 2, -1, -1):
            if i == self.num_layers - 2:  # Output layer
                delta = deltas[0]
            else:
                delta = np.dot(deltas[-1], self.weights[i+1].T) * self._activation_derivative(self.z_values[i])
            
            deltas.append(delta)
        
        deltas.reverse()
        
        # Update weights and biases
        for i in range(self.num_layers - 1):
            self.weights[i] -= learning_rate * np.dot(self.activations[i].T, deltas[i]) / m
            self.biases[i] -= learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / m
    
    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            self.backward(X, y, learning_rate)
            
            if epoch % 100 == 0:
                predictions = self.forward(X)
                loss = np.mean((predictions - y) ** 2)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def predict(self, X):
        return self.forward(X)

# Example usage
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])  # XOR problem

mlp = MLP(layer_sizes=[2, 4, 1], activation='relu')
mlp.train(X, y, epochs=1000, learning_rate=0.1)

# Test prediction
test_point = np.array([[0, 1]])
prediction = mlp.predict(test_point)
print(f"Prediction for {test_point}: {prediction[0][0]:.4f}")
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>Activation Functions</h3>
                <p>Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Different activation functions have different properties and use cases.</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>ReLU (Rectified Linear Unit)</h4>
                        <p>ReLU is the most commonly used activation function in deep learning. It's simple and computationally efficient.</p>
                        <div class="math-box">
\[
\text{ReLU}(x) = \max(0, x) = \begin{cases} 
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0 
\end{cases}
\]

Derivative:
\[
\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0 
\end{cases}
\]

Advantages:
- Computationally efficient
- Helps mitigate the vanishing gradient problem
- Induces sparsity in activations

Disadvantages:
- "Dying ReLU" problem (neurons can get stuck)
- Not zero-centered
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Sigmoid</h4>
                        <p>Sigmoid squashes input values to a range between 0 and 1, making it useful for binary classification output layers.</p>
                        <div class="math-box">
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Derivative:
\[
\sigma'(x) = \sigma(x)(1 - \sigma(x))
\]

Advantages:
- Smooth gradient
- Output bound between 0 and 1
- Clear interpretation (probability)

Disadvantages:
- Vanishing gradient problem
- Not zero-centered
- Computationally expensive
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Softmax</h4>
                        <p>Softmax is used in the output layer for multi-class classification problems. It converts logits to probabilities.</p>
                        <div class="math-box">
For a vector \(\mathbf{z} = [z_1, z_2, ..., z_K]\) of length \(K\):

\[
\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, ..., K
\]

Properties:
- Outputs sum to 1: \(\sum_{i=1}^{K} \text{softmax}(\mathbf{z})_i = 1\)
- Each output is in (0, 1)
- Preserves order: if \(z_i > z_j\), then \(\text{softmax}(\mathbf{z})_i > \text{softmax}(\mathbf{z})_j\)

Jacobian matrix for gradient computation:
\[
\frac{\partial \text{softmax}(\mathbf{z})_i}{\partial z_j} = \begin{cases} 
\text{softmax}(\mathbf{z})_i (1 - \text{softmax}(\mathbf{z})_i) & \text{if } i = j \\
-\text{softmax}(\mathbf{z})_i \text{softmax}(\mathbf{z})_j & \text{if } i \neq j 
\end{cases}
\]
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Other Activation Functions</h4>
                        <p>Several other activation functions are used in specific scenarios:</p>
                        <div class="math-box">
Tanh (Hyperbolic Tangent):
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
Range: (-1, 1)

Leaky ReLU:
\[
\text{LeakyReLU}(x) = \begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0 
\end{cases}
\]
Where \(\alpha\) is a small positive constant (e.g., 0.01)

ELU (Exponential Linear Unit):
\[
\text{ELU}(x) = \begin{cases} 
x & \text{if } x > 0 \\
\alpha (e^x - 1) & \text{if } x \leq 0 
\end{cases}
\]

Swish:
\[
\text{Swish}(x) = x \cdot \sigma(\beta x)
\]
Where \(\sigma\) is the sigmoid function and \(\beta\) is a learnable parameter.
                        </div>
                    </div>
                </div>
                
                <div class="interactive-demo">
                    <h4>Activation Functions Visualization</h4>
                    <p>Compare different activation functions and their derivatives.</p>
                    
                    <div class="tab-container">
                        <div class="tab-buttons">
                            <button class="tab-button active" data-tab="relu">ReLU</button>
                            <button class="tab-button" data-tab="sigmoid">Sigmoid</button>
                            <button class="tab-button" data-tab="softmax">Softmax</button>
                            <button class="tab-button" data-tab="others">Others</button>
                        </div>
                        
                        <div id="relu" class="tab-content active">
                            <div class="chart-container">
                                <canvas id="relu-chart"></canvas>
                            </div>
                        </div>
                        
                        <div id="sigmoid" class="tab-content">
                            <div class="chart-container">
                                <canvas id="sigmoid-chart"></canvas>
                            </div>
                        </div>
                        
                        <div id="softmax" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="softmax-inputs">Input Values (comma-separated):</label>
                                    <input type="text" id="softmax-inputs" value="1.0, 2.0, 3.0, 0.5" placeholder="e.g., 1.0, 2.0, 3.0, 0.5">
                                </div>
                                <button id="update-softmax">Update Softmax</button>
                            </div>
                            
                            <div id="softmax-result" class="code-box" style="display: none;"></div>
                            <div class="chart-container">
                                <canvas id="softmax-chart"></canvas>
                            </div>
                        </div>
                        
                        <div id="others" class="tab-content">
                            <div class="chart-container">
                                <canvas id="others-chart"></canvas>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>Backpropagation</h3>
                <p>Backpropagation is the algorithm used to train neural networks. It efficiently computes the gradient of the loss function with respect to each weight in the network.</p>
                
                <div class="math-box">
Backpropagation consists of two main phases:

1. Forward Pass: Compute the output of the network
2. Backward Pass: Compute gradients and update weights

For a network with \(L\) layers, the forward pass is:
\[
\mathbf{a}^{[0]} = \mathbf{X}
\]
\[
\mathbf{z}^{[l]} = \mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]} \quad \text{for } l = 1, 2, ..., L
\]
\[
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{z}^{[l]}) \quad \text{for } l = 1, 2, ..., L-1
\]
\[
\hat{\mathbf{y}} = \mathbf{a}^{[L]}
\]

The loss function for a single example is:
\[
\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y}) = \frac{1}{2} \|\hat{\mathbf{y}} - \mathbf{y}\|^2 \quad \text{(MSE for regression)}
\]

Backward pass starts from the output layer:
\[
\delta^{[L]} = \nabla_{\mathbf{a}^{[L]}} \mathcal{L} \odot g'^{[L]}(\mathbf{z}^{[L]})
\]

For layers \(l = L-1, L-2, ..., 1\):
\[
\delta^{[l]} = (\mathbf{W}^{[l+1]})^T \delta^{[l+1]} \odot g'^{[l]}(\mathbf{z}^{[l]})
\]

Gradients for weights and biases:
\[
\nabla_{\mathbf{W}^{[l]}} \mathcal{L} = \delta^{[l]} (\mathbf{a}^{[l-1]})^T
\]
\[
\nabla_{\mathbf{b}^{[l]}} \mathcal{L} = \delta^{[l]}
\]

Weight update using gradient descent:
\[
\mathbf{W}^{[l]} = \mathbf{W}^{[l]} - \eta \nabla_{\mathbf{W}^{[l]}} \mathcal{L}
\]
\[
\mathbf{b}^{[l]} = \mathbf{b}^{[l]} - \eta \nabla_{\mathbf{b}^{[l]}} \mathcal{L}
\]

Where:
- \(\delta^{[l]}\) is the error term for layer \(l\)
- \(\odot\) denotes element-wise multiplication
- \(\eta\) is the learning rate
                </div>
                
                <div class="interactive-demo">
                    <h4>Interactive Backpropagation Demo</h4>
                    <p>Visualize how gradients flow backward through a simple neural network.</p>
                    
                    <div class="demo-controls">
                        <div class="control-group">
                            <label for="input-value">Input Value:</label>
                            <input type="number" id="input-value" value="1" step="0.1">
                        </div>
                        <div class="control-group">
                            <label for="target-value">Target Value:</label>
                            <input type="number" id="target-value" value="0" step="0.1">
                        </div>
                        <div class="control-group">
                            <label for="learning-rate">Learning Rate:</label>
                            <input type="number" id="learning-rate" value="0.1" step="0.01" min="0.01" max="1">
                        </div>
                        <button id="run-backprop">Run Backpropagation</button>
                    </div>
                    
                    <div id="backprop-result" class="code-box" style="display: none;"></div>
                    
                    <div class="chart-container">
                        <canvas id="backprop-chart"></canvas>
                    </div>
                    
                    <div class="code-box">
# Backpropagation implementation for a simple neural network
import numpy as np

class SimpleNN:
    def __init__(self):
        # Initialize weights and biases for a 1-2-1 network
        self.W1 = np.random.randn(1, 2) * 0.1
        self.b1 = np.zeros((1, 2))
        self.W2 = np.random.randn(2, 1) * 0.1
        self.b2 = np.zeros((1, 1))
        
        # Store activations and gradients for visualization
        self.activations = {}
        self.gradients = {}
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def forward(self, X):
        # Input to hidden layer
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        
        # Hidden to output layer
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        
        # Store activations
        self.activations = {
            'X': X,
            'z1': self.z1,
            'a1': self.a1,
            'z2': self.z2,
            'a2': self.a2
        }
        
        return self.a2
    
    def backward(self, X, y, learning_rate):
        m = X.shape[0]
        
        # Compute loss (MSE)
        loss = 0.5 * np.mean((self.a2 - y) ** 2)
        
        # Output layer gradients
        dz2 = (self.a2 - y) * self.sigmoid_derivative(self.z2)
        dW2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        # Hidden layer gradients
        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.z1)
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # Store gradients
        self.gradients = {
            'dW1': dW1,
            'db1': db1,
            'dW2': dW2,
            'db2': db2,
            'dz1': dz1,
            'dz2': dz2
        }
        
        # Update weights and biases
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        
        return loss
    
    def train(self, X, y, epochs, learning_rate):
        losses = []
        
        for epoch in range(epochs):
            # Forward pass
            output = self.forward(X)
            
            # Backward pass
            loss = self.backward(X, y, learning_rate)
            losses.append(loss)
            
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")
        
        return losses

# Example usage
X = np.array([[1.0]])
y = np.array([[0.0]])

nn = SimpleNN()
losses = nn.train(X, y, epochs=1000, learning_rate=0.1)

# Test prediction
prediction = nn.forward(X)
print(f"Input: {X[0][0]}, Target: {y[0][0]}, Prediction: {prediction[0][0]:.6f}")
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Deep Learning Frameworks Section -->
        <section id="frameworks" class="section">
            <div class="section-header">
                <div class="section-icon">🔹</div>
                <h2>Deep Learning Frameworks</h2>
            </div>
            
            <div class="subsection">
                <h3>TensorFlow</h3>
                <p>TensorFlow is an open-source machine learning framework developed by Google. It provides a comprehensive ecosystem of tools for building and deploying machine learning models.</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>Core Concepts</h4>
                        <p>TensorFlow operates on tensors, which are multi-dimensional arrays with a uniform type. The computation graph is a fundamental concept in TensorFlow.</p>
                        <div class="code-box">
import tensorflow as tf

# Tensors
scalar = tf.constant(5)                      # 0-dimensional tensor
vector = tf.constant([1, 2, 3])              # 1-dimensional tensor
matrix = tf.constant([[1, 2], [3, 4]])        # 2-dimensional tensor
tensor_3d = tf.constant([[[1, 2], [3, 4]],    # 3-dimensional tensor
                        [[5, 6], [7, 8]]])

# Tensor properties
print("Scalar:", scalar)
print("Vector:", vector)
print("Matrix:", matrix)
print("3D Tensor:", tensor_3d)

# Tensor operations
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Element-wise operations
print("Addition:", a + b)
print("Multiplication:", a * b)

# Matrix multiplication
print("Matrix multiplication:", tf.matmul(a, b))

# Reduction operations
print("Sum of all elements:", tf.reduce_sum(a))
print("Mean of each column:", tf.reduce_mean(a, axis=0))
print("Max of each row:", tf.reduce_max(a, axis=1))

# Reshaping
print("Reshaped:", tf.reshape(a, [4, 1]))
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Building Models with TensorFlow</h4>
                        <p>TensorFlow provides high-level APIs like Keras for building neural networks with ease. Here's how to build a simple model for image classification.</p>
                        <div class="code-box">
import tensorflow as tf
from tensorflow.keras import layers, models, datasets
import matplotlib.pyplot as plt

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()

# Normalize pixel values to [0, 1]
train_images = train_images / 255.0
test_images = test_images / 255.0

# Build the model
model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),      # Flatten 28x28 images to 784-dimensional vectors
    layers.Dense(128, activation='relu'),       # Fully connected layer with 128 units
    layers.Dropout(0.2),                       # Dropout for regularization
    layers.Dense(10, activation='softmax')     # Output layer with 10 units (one for each digit)
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_images, train_labels, 
                    epochs=10, 
                    validation_data=(test_images, test_labels))

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f"\nTest accuracy: {test_acc:.4f}")

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Make predictions
predictions = model.predict(test_images)
print(f"Prediction for first test image: {tf.argmax(predictions[0])}")
print(f"Actual label: {test_labels[0]}")
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>PyTorch</h3>
                <p>PyTorch is an open-source machine learning framework developed by Facebook. It's known for its dynamic computation graph and ease of use.</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>Core Concepts</h4>
                        <p>PyTorch's core data structure is the tensor, similar to NumPy arrays but with GPU acceleration support. Autograd is PyTorch's automatic differentiation engine.</p>
                        <div class="code-box">
import torch
import torch.nn as nn
import torch.optim as optim

# Tensors
scalar = torch.tensor(5.0)                    # 0-dimensional tensor
vector = torch.tensor([1, 2, 3])              # 1-dimensional tensor
matrix = torch.tensor([[1, 2], [3, 4]])        # 2-dimensional tensor
tensor_3d = torch.tensor([[[1, 2], [3, 4]],    # 3-dimensional tensor
                        [[5, 6], [7, 8]]])

# Tensor properties
print("Scalar:", scalar)
print("Vector:", vector)
print("Matrix:", matrix)
print("3D Tensor:", tensor_3d)

# Tensor operations
a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)

# Element-wise operations
print("Addition:", a + b)
print("Multiplication:", a * b)

# Matrix multiplication
print("Matrix multiplication:", torch.matmul(a, b))

# Reduction operations
print("Sum of all elements:", torch.sum(a))
print("Mean of each column:", torch.mean(a, dim=0))
print("Max of each row:", torch.max(a, dim=1))

# Reshaping
print("Reshaped:", a.reshape(4, 1))

# Autograd example
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2 + 3 * x + 1
y.backward()

print("Gradient of y with respect to x:", x.grad)
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Building Models with PyTorch</h4>
                        <p>PyTorch provides a modular approach to building neural networks. Here's how to build a simple model for image classification.</p>
                        <div class="code-box">
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Load and preprocess the MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)

# Initialize the model, loss function, and optimizer
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Training function
def train(model, device, train_loader, optimizer, criterion, epoch):
    model.train()
    train_loss = 0
    correct = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
    
    train_loss /= len(train_loader.dataset)
    accuracy = 100. * correct / len(train_loader.dataset)
    print(f'Train Epoch: {epoch} \tLoss: {train_loss:.4f}\tAccuracy: {accuracy:.2f}%')
    return train_loss, accuracy

# Testing function
def test(model, device, test_loader, criterion):
    model.eval()
    test_loss = 0
    correct = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'Test set: Average loss: {test_loss:.4f}\tAccuracy: {accuracy:.2f}%')
    return test_loss, accuracy

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Train the model
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []

for epoch in range(1, 11):
    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)
    test_loss, test_acc = test(model, device, test_loader, criterion)
    
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# Make a prediction on a test sample
data, target = next(iter(test_loader))
data, target = data.to(device), target.to(device)
output = model(data)
prediction = output.argmax(dim=1)

print(f"Prediction for first test image: {prediction[0].item()}")
print(f"Actual label: {target[0].item()}")
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>Keras for Quick Prototyping</h3>
                <p>Keras is a high-level neural networks API that runs on top of TensorFlow. It provides a user-friendly interface for building and training deep learning models.</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>Keras Sequential API</h4>
                        <p>The Sequential API is the simplest way to build models in Keras. It's perfect for building simple stacks of layers.</p>
                        <div class="code-box">
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Build a model using the Sequential API
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Display model architecture
model.summary()

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Load and preprocess data
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# Train the model
history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=10,
                    validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_acc:.4f}')

# Make predictions
predictions = model.predict(x_test[:5])
predicted_classes = tf.argmax(predictions, axis=1)
print(f"Predicted classes: {predicted_classes.numpy()}")
print(f"Actual classes: {y_test[:5]}")
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Keras Functional API</h4>
                        <p>The Functional API is more flexible than the Sequential API and allows you to build more complex models with multiple inputs/outputs or shared layers.</p>
                        <div class="code-box">
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model, Input

# Build a model using the Functional API
# Input layer
inputs = Input(shape=(784,), name='digits')

# Hidden layers
x = layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = layers.Dropout(0.2, name='dropout_1')(x)
x = layers.Dense(64, activation='relu', name='dense_2')(x)
x = layers.Dropout(0.2, name='dropout_2')(x)

# Output layer
outputs = layers.Dense(10, activation='softmax', name='predictions')(x)

# Create the model
model = Model(inputs=inputs, outputs=outputs, name='mnist_model')

# Display model architecture
model.summary()

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Load and preprocess data
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# Train the model
history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=10,
                    validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_acc:.4f}')

# Save the model
model.save('mnist_model.h5')

# Load the model
loaded_model = keras.models.load_model('mnist_model.h5')

# Make predictions with the loaded model
predictions = loaded_model.predict(x_test[:5])
predicted_classes = tf.argmax(predictions, axis=1)
print(f"Predicted classes: {predicted_classes.numpy()}")
print(f"Actual classes: {y_test[:5]}")
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>GPU Acceleration</h3>
                <p>GPU acceleration is crucial for training deep learning models efficiently. Here's how to leverage GPUs with TensorFlow, PyTorch, and Google Colab.</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>TensorFlow GPU Setup</h4>
                        <p>TensorFlow automatically uses GPU if available. Here's how to check and control GPU usage.</p>
                        <div class="code-box">
import tensorflow as tf

# Check if GPU is available
print("GPU Available:", tf.test.is_gpu_available())
print("GPU Device Name:", tf.test.gpu_device_name())

# List available physical devices
physical_devices = tf.config.list_physical_devices()
print("Physical Devices:", physical_devices)

# List available GPUs
gpus = tf.config.list_physical_devices('GPU')
print("GPUs:", gpus)

# Enable memory growth to prevent TensorFlow from allocating all memory at once
if gpus:
    try:
        # Currently, memory growth needs to be set before GPUs have been initialized
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("Memory growth set for GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

# Place operations on specific devices
with tf.device('/CPU:0'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

with tf.device('/GPU:0' if gpus else '/CPU:0'):
    c = tf.matmul(a, b)

print("Matrix multiplication result:", c)

# Time operations on CPU vs GPU
import time

# Create large matrices
size = 5000
matrix_a = tf.random.normal((size, size))
matrix_b = tf.random.normal((size, size))

# CPU computation
start_time = time.time()
with tf.device('/CPU:0'):
    cpu_result = tf.matmul(matrix_a, matrix_b)
cpu_time = time.time() - start_time
print(f"CPU computation time: {cpu_time:.4f} seconds")

# GPU computation (if available)
if gpus:
    start_time = time.time()
    with tf.device('/GPU:0'):
        gpu_result = tf.matmul(matrix_a, matrix_b)
    gpu_time = time.time() - start_time
    print(f"GPU computation time: {gpu_time:.4f} seconds")
    print(f"Speedup: {cpu_time / gpu_time:.2f}x")
else:
    print("No GPU available for comparison")
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>PyTorch GPU Setup</h4>
                        <p>PyTorch provides explicit control over device placement. Here's how to use GPUs with PyTorch.</p>
                        <div class="code-box">
import torch
import torch.nn as nn
import time

# Check if CUDA is available
print("CUDA Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA Device Name:", torch.cuda.get_device_name(0))
    print("CUDA Device Count:", torch.cuda.device_count())

# Get the current device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Create tensors on specific devices
# CPU tensor
cpu_tensor = torch.randn(1000, 1000)
print("CPU tensor device:", cpu_tensor.device)

# GPU tensor (if available)
if torch.cuda.is_available():
    gpu_tensor = torch.randn(1000, 1000).to(device)
    print("GPU tensor device:", gpu_tensor.device)

# Move tensors between devices
if torch.cuda.is_available():
    # Move CPU tensor to GPU
    tensor_on_gpu = cpu_tensor.to(device)
    print("Moved to GPU:", tensor_on_gpu.device)
    
    # Move GPU tensor back to CPU
    tensor_on_cpu = tensor_on_gpu.cpu()
    print("Moved back to CPU:", tensor_on_cpu.device)

# Define a simple neural network
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(1000, 500)
        self.fc2 = nn.Linear(500, 100)
        self.fc3 = nn.Linear(100, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Create model and move to device
model = SimpleNet().to(device)
print("Model device:", next(model.parameters()).device)

# Time operations on CPU vs GPU
input_size = 5000
input_data = torch.randn(input_size, 1000)
target = torch.randn(input_size, 10)

# CPU computation
start_time = time.time()
model_cpu = SimpleNet()
output_cpu = model_cpu(input_data)
cpu_time = time.time() - start_time
print(f"CPU computation time: {cpu_time:.4f} seconds")

# GPU computation (if available)
if torch.cuda.is_available():
    input_gpu = input_data.to(device)
    target_gpu = target.to(device)
    
    start_time = time.time()
    output_gpu = model(input_gpu)
    gpu_time = time.time() - start_time
    print(f"GPU computation time: {gpu_time:.4f} seconds")
    print(f"Speedup: {cpu_time / gpu_time:.2f}x")
else:
    print("No GPU available for comparison")
                        </div>
                    </div>
                    
                    <div class="card">
                        <h4>Google Colab</h4>
                        <p>Google Colab provides free GPU access for deep learning experiments. Here's how to set up and use Colab effectively.</p>
                        <div class="code-box">
# Google Colab GPU setup

# Check GPU availability in Colab
!nvidia-smi

# Install required packages
!pip install tensorflow torch torchvision

# Import libraries
import tensorflow as tf
import torch
import numpy as np

# Check TensorFlow GPU
print("TensorFlow version:", tf.__version__)
print("GPU Available:", tf.test.is_gpu_available())

# Check PyTorch GPU
print("PyTorch version:", torch.__version__)
print("CUDA Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA Device Name:", torch.cuda.get_device_name(0))

# Mount Google Drive for data storage
from google.colab import drive
drive.mount('/content/drive')

# Create a directory in your Drive
!mkdir -p "/content/drive/My Drive/Colab Notebooks/models"

# Example: Training a model in Colab
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Build a simple model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# Train the model
history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=10,
                    validation_split=0.2)

# Save the model to Google Drive
model.save("/content/drive/My Drive/Colab Notebooks/models/mnist_model.h5")
print("Model saved to Google Drive")

# Load the model from Google Drive
loaded_model = keras.models.load_model("/content/drive/My Drive/Colab Notebooks/models/mnist_model.h5")

# Evaluate the loaded model
test_loss, test_acc = loaded_model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

# Plot training history
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- CNNs, RNNs, LSTMs Section -->
        <section id="cnn-rnn" class="section">
            <div class="section-header">
                <div class="section-icon">🔹</div>
                <h2>CNNs, RNNs, LSTMs</h2>
            </div>
            
            <div class="subsection">
                <h3>Computer Vision with CNNs</h3>
                <p>Convolutional Neural Networks (CNNs) are specialized neural networks designed for processing grid-like data such as images. They're highly effective for computer vision tasks.</p>
                
                <div class="math-box">
CNNs consist of several types of layers:

1. Convolutional Layer:
\[
(I * K)(i,j) = \sum_{m} \sum_{n} I(i+m, j+n) K(m,n)
\]

Where:
- \(I\) is the input image
- \(K\) is the kernel or filter
- \((i,j)\) are the position indices

2. Activation Function (typically ReLU):
\[
f(x) = \max(0, x)
\]

3. Pooling Layer (Max Pooling):
\[
y_{i,j} = \max_{0 \leq m,n < s} x_{i \cdot s + m, j \cdot s + n}
\]

Where:
- \(s\) is the stride size
- \(x\) is the input feature map
- \(y\) is the output feature map

4. Fully Connected Layer:
\[
y = f(Wx + b)
\]

Where:
- \(W\) is the weight matrix
- \(b\) is the bias vector
- \(f\) is the activation function

The forward pass of a CNN can be represented as:
\[
x^{(l)} = f^{(l)}(W^{(l)} * x^{(l-1)} + b^{(l)})
\]

For convolutional layers, and:
\[
x^{(l)} = f^{(l)}(W^{(l)} x^{(l-1)} + b^{(l)})
\]

For fully connected layers.

Key concepts:
- Local Connectivity: Each neuron in a convolutional layer is connected only to a local region of the input
- Parameter Sharing: The same filter is used across different positions of the input
- Spatial Hierarchy: Deeper layers capture more abstract features
                </div>
                
                <div class="interactive-demo">
                    <h4>Interactive CNN Demo</h4>
                    <p>Visualize how a CNN processes an image through convolutional and pooling layers.</p>
                    
                    <div class="tab-container">
                        <div class="tab-buttons">
                            <button class="tab-button active" data-tab="convolution">Convolution</button>
                            <button class="tab-button" data-tab="pooling">Pooling</button>
                            <button class="tab-button" data-tab="cnn-model">CNN Model</button>
                        </div>
                        
                        <div id="convolution" class="tab-content active">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="kernel-size">Kernel Size:</label>
                                    <select id="kernel-size">
                                        <option value="3">3x3</option>
                                        <option value="5">5x5</option>
                                        <option value="7">7x7</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="stride">Stride:</label>
                                    <select id="stride">
                                        <option value="1">1</option>
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="padding">Padding:</label>
                                    <select id="padding">
                                        <option value="valid">Valid</option>
                                        <option value="same">Same</option>
                                    </select>
                                </div>
                                <button id="apply-convolution">Apply Convolution</button>
                            </div>
                            
                            <div id="convolution-result" class="code-box" style="display: none;"></div>
                            <div class="chart-container">
                                <canvas id="convolution-chart"></canvas>
                            </div>
                        </div>
                        
                        <div id="pooling" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="pool-size">Pool Size:</label>
                                    <select id="pool-size">
                                        <option value="2">2x2</option>
                                        <option value="3">3x3</option>
                                        <option value="4">4x4</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="pool-type">Pool Type:</label>
                                    <select id="pool-type">
                                        <option value="max">Max Pooling</option>
                                        <option value="avg">Average Pooling</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="pool-stride">Stride:</label>
                                    <select id="pool-stride">
                                        <option value="1">1</option>
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                    </select>
                                </div>
                                <button id="apply-pooling">Apply Pooling</button>
                            </div>
                            
                            <div id="pooling-result" class="code-box" style="display: none;"></div>
                            <div class="chart-container">
                                <canvas id="pooling-chart"></canvas>
                            </div>
                        </div>
                        
                        <div id="cnn-model" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="dataset">Dataset:</label>
                                    <select id="dataset">
                                        <option value="mnist">MNIST</option>
                                        <option value="cifar10">CIFAR-10</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="conv-layers">Conv Layers:</label>
                                    <select id="conv-layers">
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                        <option value="4">4</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="filters">Filters per Layer:</label>
                                    <select id="filters">
                                        <option value="32">32</option>
                                        <option value="64">64</option>
                                        <option value="128">128</option>
                                    </select>
                                </div>
                                <button id="build-cnn">Build CNN</button>
                            </div>
                            
                            <div id="cnn-architecture" class="code-box" style="display: none;"></div>
                        </div>
                    </div>
                    
                    <div class="code-box">
# CNN implementation for image classification
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, datasets
import matplotlib.pyplot as plt
import numpy as np

# Load and preprocess the CIFAR-10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to [0, 1]
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0

# Convert labels to one-hot encoding
train_labels = keras.utils.to_categorical(train_labels, 10)
test_labels = keras.utils.to_categorical(test_labels, 10)

# Define class names for CIFAR-10
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Build the CNN model
def build_cnn(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential([
        # Convolutional base
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Classifier
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

# Build and compile the model
model = build_cnn()
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Display model architecture
model.summary()

# Define data augmentation
datagen = keras.preprocessing.image.ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1
)

# Train the model with data augmentation
batch_size = 64
epochs = 50

history = model.fit(
    datagen.flow(train_images, train_labels, batch_size=batch_size),
    steps_per_epoch=len(train_images) / batch_size,
    epochs=epochs,
    validation_data=(test_images, test_labels),
    verbose=1
)

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'Test accuracy: {test_acc:.4f}')

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Make predictions on test images
predictions = model.predict(test_images)

# Plot some test images with their predictions
plt.figure(figsize=(15, 15))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(test_images[i])
    
    predicted_label = np.argmax(predictions[i])
    true_label = np.argmax(test_labels[i])
    
    if predicted_label == true_label:
        color = 'green'
    else:
        color = 'red'
    
    plt.xlabel(f"{class_names[predicted_label]} ({class_names[true_label]})",
               color=color)

plt.show()

# Save the model
model.save('cifar10_cnn.h5')
print("Model saved as 'cifar10_cnn.h5'")
                    </div>
                </div>
            </div>
            
            <div class="subsection">
                <h3>Sequence Modeling with RNNs, LSTMs</h3>
                <p>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are designed for processing sequential data like text, time series, or speech.</p>
                
                <div class="math-box">
RNNs process sequences by maintaining a hidden state that captures information from previous time steps:

For a simple RNN:
\[
h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
\]
\[
y_t = W_{hy} h_t + b_y
\]

Where:
- \(x_t\) is the input at time step \(t\)
- \(h_t\) is the hidden state at time step \(t\)
- \(y_t\) is the output at time step \(t\)
- \(W_{xh}\), \(W_{hh}\), and \(W_{hy}\) are weight matrices
- \(b_h\) and \(b_y\) are bias vectors

LSTMs address the vanishing gradient problem in RNNs by using a more complex cell structure with gates:

\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\]
\[
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\]
\[
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
\]
\[
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\]
\[
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\]
\[
h_t = o_t \odot \tanh(C_t)
\]

Where:
- \(f_t\) is the forget gate
- \(i_t\) is the input gate
- \(\tilde{C}_t\) is the candidate cell state
- \(C_t\) is the cell state
- \(o_t\) is the output gate
- \(\sigma\) is the sigmoid function
- \(\odot\) denotes element-wise multiplication

Gated Recurrent Units (GRUs) are a simplified version of LSTMs:

\[
z_t = \sigma(W_z \cdot [h_{t-1}, x_t])
\]
\[
r_t = \sigma(W_r \cdot [h_{t-1}, x_t])
\]
\[
\tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t])
\]
\[
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\]

Where:
- \(z_t\) is the update gate
- \(r_t\) is the reset gate
- \(\tilde{h}_t\) is the candidate hidden state
                </div>
                
                <div class="interactive-demo">
                    <h4>Interactive RNN/LSTM Demo</h4>
                    <p>Explore how RNNs and LSTMs process sequential data.</p>
                    
                    <div class="tab-container">
                        <div class="tab-buttons">
                            <button class="tab-button active" data-tab="rnn">RNN</button>
                            <button class="tab-button" data-tab="lstm">LSTM</button>
                            <button class="tab-button" data-tab="gru">GRU</button>
                            <button class="tab-button" data-tab="sequence-model">Sequence Model</button>
                        </div>
                        
                        <div id="rnn" class="tab-content active">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="rnn-sequence">Sequence Length:</label>
                                    <select id="rnn-sequence">
                                        <option value="5">5</option>
                                        <option value="10">10</option>
                                        <option value="20">20</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="rnn-hidden">Hidden Size:</label>
                                    <select id="rnn-hidden">
                                        <option value="32">32</option>
                                        <option value="64">64</option>
                                        <option value="128">128</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="rnn-layers">Number of Layers:</label>
                                    <select id="rnn-layers">
                                        <option value="1">1</option>
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                    </select>
                                </div>
                                <button id="visualize-rnn">Visualize RNN</button>
                            </div>
                            
                            <div id="rnn-visualization" class="neural-network"></div>
                            <div id="rnn-result" class="code-box" style="display: none;"></div>
                        </div>
                        
                        <div id="lstm" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="lstm-sequence">Sequence Length:</label>
                                    <select id="lstm-sequence">
                                        <option value="5">5</option>
                                        <option value="10">10</option>
                                        <option value="20">20</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="lstm-hidden">Hidden Size:</label>
                                    <select id="lstm-hidden">
                                        <option value="32">32</option>
                                        <option value="64">64</option>
                                        <option value="128">128</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="lstm-layers">Number of Layers:</label>
                                    <select id="lstm-layers">
                                        <option value="1">1</option>
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                    </select>
                                </div>
                                <button id="visualize-lstm">Visualize LSTM</button>
                            </div>
                            
                            <div id="lstm-visualization" class="neural-network"></div>
                            <div id="lstm-result" class="code-box" style="display: none;"></div>
                        </div>
                        
                        <div id="gru" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="gru-sequence">Sequence Length:</label>
                                    <select id="gru-sequence">
                                        <option value="5">5</option>
                                        <option value="10">10</option>
                                        <option value="20">20</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="gru-hidden">Hidden Size:</label>
                                    <select id="gru-hidden">
                                        <option value="32">32</option>
                                        <option value="64">64</option>
                                        <option value="128">128</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="gru-layers">Number of Layers:</label>
                                    <select id="gru-layers">
                                        <option value="1">1</option>
                                        <option value="2">2</option>
                                        <option value="3">3</option>
                                    </select>
                                </div>
                                <button id="visualize-gru">Visualize GRU</button>
                            </div>
                            
                            <div id="gru-visualization" class="neural-network"></div>
                            <div id="gru-result" class="code-box" style="display: none;"></div>
                        </div>
                        
                        <div id="sequence-model" class="tab-content">
                            <div class="demo-controls">
                                <div class="control-group">
                                    <label for="task-type">Task Type:</label>
                                    <select id="task-type">
                                        <option value="text-classification">Text Classification</option>
                                        <option value="sentiment-analysis">Sentiment Analysis</option>
                                        <option value="time-series">Time Series Prediction</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="model-type">Model Type:</label>
                                    <select id="model-type">
                                        <option value="rnn">RNN</option>
                                        <option value="lstm">LSTM</option>
                                        <option value="gru">GRU</option>
                                        <option value="bidirectional">Bidirectional LSTM</option>
                                    </select>
                                </div>
                                <div class="control-group">
                                    <label for="embedding-dim">Embedding Dimension:</label>
                                    <select id="embedding-dim">
                                        <option value="50">50</option>
                                        <option value="100">100</option>
                                        <option value="200">200</option>
                                    </select>
                                </div>
                                <button id="build-sequence-model">Build Model</button>
                            </div>
                            
                            <div id="sequence-model-code" class="code-box" style="display: none;"></div>
                        </div>
                    </div>
                    
                    <div class="code-box">
# LSTM implementation for text classification
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, datasets, preprocessing
import matplotlib.pyplot as plt

# Load the IMDB dataset
max_features = 10000  # Number of words to consider as features
maxlen = 200  # Cut texts after this number of words

(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(
    num_words=max_features)
print(f"Training sequences: {len(x_train)}, Test sequences: {len(x_test)}")

# Pad sequences to ensure they all have the same length
x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)
print(f"x_train shape: {x_train.shape}")
print(f"x_test shape: {x_test.shape}")

# Build the LSTM model
def build_lstm_model(max_features, maxlen, embedding_dim=128):
    model = models.Sequential([
        # Embedding layer
        layers.Embedding(max_features, embedding_dim, input_length=maxlen),
        
        # LSTM layer
        layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),
        
        # Dense layers
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')
    ])
    
    return model

# Build and compile the model
model = build_lstm_model(max_features, maxlen)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Display model architecture
model.summary()

# Train the model
batch_size = 32
epochs = 10

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_data=(x_test, y_test),
                    verbose=1)

# Evaluate the model
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f'Test accuracy: {test_acc:.4f}')

# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Make predictions on test data
predictions = model.predict(x_test)

# Convert predictions to binary (0 or 1)
binary_predictions = (predictions > 0.5).astype(int)

# Show some predictions
print("Sample predictions:")
for i in range(10):
    print(f"Review {i+1}: Predicted={binary_predictions[i][0]}, Actual={y_test[i]}")

# Save the model
model.save('imdb_lstm.h5')
print("Model saved as 'imdb_lstm.h5'")

# Build a Bidirectional LSTM model
def build_bidirectional_lstm_model(max_features, maxlen, embedding_dim=128):
    model = models.Sequential([
        # Embedding layer
        layers.Embedding(max_features, embedding_dim, input_length=maxlen),
        
        # Bidirectional LSTM layer
        layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),
        
        # Dense layers
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(1, activation='sigmoid')
    ])
    
    return model

# Build and compile the bidirectional model
bi_model = build_bidirectional_lstm_model(max_features, maxlen)
bi_model.compile(optimizer='adam',
                 loss='binary_crossentropy',
                 metrics=['accuracy'])

# Train the bidirectional model
bi_history = bi_model.fit(x_train, y_train,
                          batch_size=batch_size,
                          epochs=epochs,
                          validation_data=(x_test, y_test),
                          verbose=1)

# Evaluate the bidirectional model
bi_test_loss, bi_test_acc = bi_model.evaluate(x_test, y_test, verbose=2)
print(f'Bidirectional LSTM Test accuracy: {bi_test_acc:.4f}')

# Compare the two models
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['val_accuracy'], label='LSTM')
plt.plot(bi_history.history['val_accuracy'], label='Bidirectional LSTM')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['val_loss'], label='LSTM')
plt.plot(bi_history.history['val_loss'], label='Bidirectional LSTM')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.legend()

plt.show()
                    </div>
                </div>
            </div>
            
            <div class="business-case">
                <h4>💼 Business Case</h4>
                <p>Deep learning models have numerous business applications across industries:</p>
                
                <div class="content-grid">
                    <div class="card">
                        <h4>Computer Vision Applications</h4>
                        <ul>
                            <li><strong>Medical Imaging:</strong> CNNs can detect diseases in X-rays, MRIs, and CT scans with high accuracy</li>
                            <li><strong>Autonomous Vehicles:</strong> CNNs process camera feeds to identify objects, pedestrians, and road signs</li>
                            <li><strong>Retail:</strong> Visual search systems allow customers to find products by uploading images</li>
                            <li><strong>Manufacturing:</strong> Automated quality control using CNNs to detect defects in products</li>
                        </ul>
                    </div>
                    
                    <div class="card">
                        <h4>Sequence Modeling Applications</h4>
                        <ul>
                            <li><strong>Customer Service:</strong> LSTMs power chatbots and virtual assistants that understand natural language</li>
                            <li><strong>Finance:</strong> RNNs and LSTMs predict stock prices and detect fraudulent transactions</li>
                            <li><strong>Healthcare:</strong> Sequential analysis of patient data to predict disease progression</li>
                            <li><strong>Content Recommendation:</strong> RNNs analyze user behavior to recommend personalized content</li>
                        </ul>
                    </div>
                </div>
                
                <div class="step">
                    <h4>Implementation Steps for Deep Learning Projects</h4>
                    <ol>
                        <li><strong>Problem Definition:</strong> Clearly define the business problem and success metrics</li>
                        <li><strong>Data Collection:</strong> Gather relevant data and ensure it's properly labeled</li>
                        <li><strong>Data Preprocessing:</strong> Clean, normalize, and prepare data for the model</li>
                        <li><strong>Model Selection:</strong> Choose appropriate architecture (CNN for images, RNN/LSTM for sequences)</li>
                        <li><strong>Model Training:</strong> Train the model with proper validation and hyperparameter tuning</li>
                        <li><strong>Model Evaluation:</strong> Evaluate performance on test data and compare to baselines</li>
                        <li><strong>Deployment:</strong> Deploy the model to production with monitoring and maintenance</li>
                    </ol>
                </div>
            </div>
        </section>
    </div>
    
    <footer>
        <div class="container">
            <p>© 2023 Deep Learning - Interactive Guide | Educational Platform</p>
        </div>
    </footer>
    
    <script>
        // MathJax configuration
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
        
        // Tab functionality
        document.querySelectorAll('.tab-button').forEach(button => {
            button.addEventListener('click', () => {
                const tabId = button.getAttribute('data-tab');
                
                // Deactivate all tabs and contents
                button.parentElement.querySelectorAll('.tab-button').forEach(btn => btn.classList.remove('active'));
                button.parentElement.parentElement.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));
                
                // Activate selected tab and content
                button.classList.add('active');
                document.getElementById(tabId).classList.add('active');
            });
        });
        
        // Perceptron demo
        const weight1Slider = document.getElementById('weight1');
        const weight2Slider = document.getElementById('weight2');
        const biasSlider = document.getElementById('bias');
        const weight1Value = document.getElementById('weight1-value');
        const weight2Value = document.getElementById('weight2-value');
        const biasValue = document.getElementById('bias-value');
        const updatePerceptronBtn = document.getElementById('update-perceptron');
        
        // Update slider values
        weight1Slider.addEventListener('input', () => {
            weight1Value.textContent = weight1Slider.value;
        });
        
        weight2Slider.addEventListener('input', () => {
            weight2Value.textContent = weight2Slider.value;
        });
        
        biasSlider.addEventListener('input', () => {
            biasValue.textContent = biasSlider.value;
        });
        
        // Perceptron chart
        const perceptronCtx = document.getElementById('perceptron-chart').getContext('2d');
        const perceptronChart = new Chart(perceptronCtx, {
            type: 'scatter',
            data: {
                datasets: [
                    {
                        label: 'Class 0',
                        data: [
                            {x: 1, y: 1},
                            {x: 2, y: 1},
                            {x: 1, y: 2}
                        ],
                        backgroundColor: 'rgba(255, 99, 132, 0.7)',
                        pointRadius: 8
                    },
                    {
                        label: 'Class 1',
                        data: [
                            {x: 4, y: 4},
                            {x: 5, y: 5},
                            {x: 4, y: 5}
                        ],
                        backgroundColor: 'rgba(54, 162, 235, 0.7)',
                        pointRadius: 8
                    },
                    {
                        label: 'Decision Boundary',
                        data: [],
                        type: 'line',
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 2,
                        pointRadius: 0,
                        fill: false
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: {
                        min: 0,
                        max: 6,
                        title: {
                            display: true,
                            text: 'Feature 1'
                        }
                    },
                    y: {
                        min: 0,
                        max: 6,
                        title: {
                            display: true,
                            text: 'Feature 2'
                        }
                    }
                }
            }
        });
        
        // Update perceptron
        updatePerceptronBtn.addEventListener('click', () => {
            const w1 = parseFloat(weight1Slider.value);
            const w2 = parseFloat(weight2Slider.value);
            const b = parseFloat(biasSlider.value);
            
            // Calculate decision boundary line: w1*x + w2*y + b = 0
            // => y = (-w1*x - b) / w2
            const boundaryData = [];
            for (let x = 0; x <= 6; x += 0.1) {
                const y = (-w1 * x - b) / w2;
                if (y >= 0 && y <= 6) {
                    boundaryData.push({x: x, y: y});
                }
            }
            
            perceptronChart.data.datasets[2].data = boundaryData;
            perceptronChart.update();
        });
        
        // Initialize perceptron
        updatePerceptronBtn.click();
        
        // MLP visualization
        const inputNeurons = document.getElementById('input-neurons');
        const hiddenLayers = document.getElementById('hidden-layers');
        const hiddenNeurons = document.getElementById('hidden-neurons');
        const outputNeurons = document.getElementById('output-neurons');
        const updateMlpBtn = document.getElementById('update-mlp');
        
        updateMlpBtn.addEventListener('click', () => {
            const inputCount = parseInt(inputNeurons.value);
            const hiddenCount = parseInt(hiddenLayers.value);
            const hiddenNeuronCount = parseInt(hiddenNeurons.value);
            const outputCount = parseInt(outputNeurons.value);
            
            const mlpVisualization = document.getElementById('mlp-visualization');
            mlpVisualization.innerHTML = '';
            
            // Create input layer
            const inputLayer = document.createElement('div');
            inputLayer.className = 'layer';
            inputLayer.innerHTML = '<h4>Input Layer</h4>';
            
            for (let i = 0; i < inputCount; i++) {
                const neuron = document.createElement('div');
                neuron.className = 'neuron';
                neuron.textContent = `I${i+1}`;
                inputLayer.appendChild(neuron);
            }
            
            mlpVisualization.appendChild(inputLayer);
            
            // Create connection and hidden layers
            let prevLayer = inputLayer;
            let prevNeuronCount = inputCount;
            
            for (let l = 0; l < hiddenCount; l++) {
                // Create connection
                const connection = document.createElement('div');
                connection.className = 'connection';
                
                const weightLabel = document.createElement('div');
                weightLabel.className = 'connection-weight';
                weightLabel.textContent = 'W';
                connection.appendChild(weightLabel);
                
                mlpVisualization.appendChild(connection);
                
                // Create hidden layer
                const hiddenLayer = document.createElement('div');
                hiddenLayer.className = 'layer';
                hiddenLayer.innerHTML = `<h4>Hidden Layer ${l+1}</h4>`;
                
                for (let i = 0; i < hiddenNeuronCount; i++) {
                    const neuron = document.createElement('div');
                    neuron.className = 'neuron';
                    neuron.textContent = `H${l+1}-${i+1}`;
                    hiddenLayer.appendChild(neuron);
                }
                
                mlpVisualization.appendChild(hiddenLayer);
                prevLayer = hiddenLayer;
                prevNeuronCount = hiddenNeuronCount;
            }
            
            // Create connection to output layer
            const connection = document.createElement('div');
            connection.className = 'connection';
            
            const weightLabel = document.createElement('div');
            weightLabel.className = 'connection-weight';
            weightLabel.textContent = 'W';
            connection.appendChild(weightLabel);
            
            mlpVisualization.appendChild(connection);
            
            // Create output layer
            const outputLayer = document.createElement('div');
            outputLayer.className = 'layer';
            outputLayer.innerHTML = '<h4>Output Layer</h4>';
            
            for (let i = 0; i < outputCount; i++) {
                const neuron = document.createElement('div');
                neuron.className = 'neuron';
                neuron.textContent = `O${i+1}`;
                outputLayer.appendChild(neuron);
            }
            
            mlpVisualization.appendChild(outputLayer);
        });
        
        // Initialize MLP visualization
        updateMlpBtn.click();
        
        // Activation functions visualization
        // ReLU chart
        const reluCtx = document.getElementById('relu-chart').getContext('2d');
        const reluChart = new Chart(reluCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [
                    {
                        label: 'ReLU',
                        data: [],
                        borderColor: 'rgba(255, 99, 132, 1)',
                        backgroundColor: 'rgba(255, 99, 132, 0.2)',
                        borderWidth: 2,
                        fill: true
                    },
                    {
                        label: 'Derivative',
                        data: [],
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 2,
                        borderDash: [5, 5],
                        fill: false
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'x'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'f(x)'
                        }
                    }
                }
            }
        });
        
        // Generate ReLU data
        const reluX = [];
        const reluY = [];
        const reluDeriv = [];
        
        for (let x = -5; x <= 5; x += 0.1) {
            reluX.push(x);
            reluY.push(Math.max(0, x));
            reluDeriv.push(x > 0 ? 1 : 0);
        }
        
        reluChart.data.labels = reluX;
        reluChart.data.datasets[0].data = reluY;
        reluChart.data.datasets[1].data = reluDeriv;
        reluChart.update();
        
        // Sigmoid chart
        const sigmoidCtx = document.getElementById('sigmoid-chart').getContext('2d');
        const sigmoidChart = new Chart(sigmoidCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [
                    {
                        label: 'Sigmoid',
                        data: [],
                        borderColor: 'rgba(255, 99, 132, 1)',
                        backgroundColor: 'rgba(255, 99, 132, 0.2)',
                        borderWidth: 2,
                        fill: true
                    },
                    {
                        label: 'Derivative',
                        data: [],
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 2,
                        borderDash: [5, 5],
                        fill: false
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'x'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'f(x)'
                        }
                    }
                }
            }
        });
        
        // Generate Sigmoid data
        const sigmoidX = [];
        const sigmoidY = [];
        const sigmoidDeriv = [];
        
        for (let x = -5; x <= 5; x += 0.1) {
            const s = 1 / (1 + Math.exp(-x));
            sigmoidX.push(x);
            sigmoidY.push(s);
            sigmoidDeriv.push(s * (1 - s));
        }
        
        sigmoidChart.data.labels = sigmoidX;
        sigmoidChart.data.datasets[0].data = sigmoidY;
        sigmoidChart.data.datasets[1].data = sigmoidDeriv;
        sigmoidChart.update();
        
        // Softmax demo
        const softmaxInputs = document.getElementById('softmax-inputs');
        const updateSoftmaxBtn = document.getElementById('update-softmax');
        const softmaxResult = document.getElementById('softmax-result');
        const softmaxCtx = document.getElementById('softmax-chart').getContext('2d');
        
        const softmaxChart = new Chart(softmaxCtx, {
            type: 'bar',
            data: {
                labels: [],
                datasets: [{
                    label: 'Softmax Output',
                    data: [],
                    backgroundColor: 'rgba(54, 162, 235, 0.7)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Probability'
                        }
                    }
                }
            }
        });
        
        updateSoftmaxBtn.addEventListener('click', () => {
            const inputs = softmaxInputs.value.split(',').map(x => parseFloat(x.trim()));
            
            // Calculate softmax
            const expInputs = inputs.map(x => Math.exp(x));
            const sumExp = expInputs.reduce((a, b) => a + b, 0);
            const softmaxOutputs = expInputs.map(x => x / sumExp);
            
            // Update result box
            let resultText = 'Softmax Calculation:\n\n';
            resultText += `Inputs: [${inputs.join(', ')}]\n\n`;
            resultText += 'Exponentials: [' + expInputs.map(x => x.toFixed(4)).join(', ') + ']\n';
            resultText += 'Sum of exponentials: ' + sumExp.toFixed(4) + '\n\n';
            resultText += 'Softmax outputs: [' + softmaxOutputs.map(x => x.toFixed(4)).join(', ') + ']\n';
            resultText += 'Sum of outputs: ' + softmaxOutputs.reduce((a, b) => a + b, 0).toFixed(4);
            
            softmaxResult.textContent = resultText;
            softmaxResult.style.display = 'block';
            
            // Update chart
            const labels = inputs.map((_, i) => `Class ${i+1}`);
            softmaxChart.data.labels = labels;
            softmaxChart.data.datasets[0].data = softmaxOutputs;
            softmaxChart.update();
        });
        
        // Initialize softmax
        updateSoftmaxBtn.click();
        
        // Other activation functions chart
        const othersCtx = document.getElementById('others-chart').getContext('2d');
        const othersChart = new Chart(othersCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [
                    {
                        label: 'Tanh',
                        data: [],
                        borderColor: 'rgba(255, 99, 132, 1)',
                        borderWidth: 2,
                        fill: false
                    },
                    {
                        label: 'Leaky ReLU (α=0.1)',
                        data: [],
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 2,
                        fill: false
                    },
                    {
                        label: 'ELU (α=1)',
                        data: [],
                        borderColor: 'rgba(75, 192, 192, 1)',
                        borderWidth: 2,
                        fill: false
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'x'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'f(x)'
                        }
                    }
                }
            }
        });
        
        // Generate other activation functions data
        const othersX = [];
        const tanhY = [];
        const leakyReluY = [];
        const eluY = [];
        
        for (let x = -5; x <= 5; x += 0.1) {
            othersX.push(x);
            tanhY.push(Math.tanh(x));
            leakyReluY.push(x > 0 ? x : 0.1 * x);
            eluY.push(x > 0 ? x : Math.exp(x) - 1);
        }
        
        othersChart.data.labels = othersX;
        othersChart.data.datasets[0].data = tanhY;
        othersChart.data.datasets[1].data = leakyReluY;
        othersChart.data.datasets[2].data = eluY;
        othersChart.update();
        
        // Backpropagation demo
        const inputValue = document.getElementById('input-value');
        const targetValue = document.getElementById('target-value');
        const learningRate = document.getElementById('learning-rate');
        const runBackpropBtn = document.getElementById('run-backprop');
        const backpropResult = document.getElementById('backprop-result');
        const backpropCtx = document.getElementById('backprop-chart').getContext('2d');
        
        const backpropChart = new Chart(backpropCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [
                    {
                        label: 'Loss',
                        data: [],
                        borderColor: 'rgba(255, 99, 132, 1)',
                        backgroundColor: 'rgba(255, 99, 132, 0.2)',
                        borderWidth: 2,
                        fill: true
                    }
                ]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Epoch'
                        }
                    },
                    y: {
                        title: {
                            display: true,
                            text: 'Loss'
                        }
                    }
                }
            }
        });
        
        runBackpropBtn.addEventListener('click', () => {
            const x = parseFloat(inputValue.value);
            const target = parseFloat(targetValue.value);
            const lr = parseFloat(learningRate.value);
            
            // Simple neural network: 1-2-1
            let w1 = Math.random() * 0.1 - 0.05;  // Small random weights
            let w2 = Math.random() * 0.1 - 0.05;
            let w3 = Math.random() * 0.1 - 0.05;
            let w4 = Math.random() * 0.1 - 0.05;
            let w5 = Math.random() * 0.1 - 0.05;
            let w6 = Math.random() * 0.1 - 0.05;
            
            let b1 = 0;
            let b2 = 0;
            let b3 = 0;
            
            const epochs = 100;
            const losses = [];
            
            let resultText = `Backpropagation Demo\n`;
            resultText += `Input: ${x}, Target: ${target}, Learning Rate: ${lr}\n\n`;
            resultText += `Initial weights:\n`;
            resultText += `w1=${w1.toFixed(4)}, w2=${w2.toFixed(4)}, w3=${w3.toFixed(4)}\n`;
            resultText += `w4=${w4.toFixed(4)}, w5=${w5.toFixed(4)}, w6=${w6.toFixed(4)}\n\n`;
            
            for (let epoch = 0; epoch < epochs; epoch++) {
                // Forward pass
                const h1 = 1 / (1 + Math.exp(-(w1 * x + b1)));
                const h2 = 1 / (1 + Math.exp(-(w2 * x + b2)));
                const output = 1 / (1 + Math.exp(-(w3 * h1 + w4 * h2 + b3)));
                
                // Calculate loss (MSE)
                const loss = 0.5 * Math.pow(output - target, 2);
                losses.push(loss);
                
                // Backward pass
                const d_output = (output - target) * output * (1 - output);
                const d_h1 = d_output * w3 * h1 * (1 - h1);
                const d_h2 = d_output * w4 * h2 * (1 - h2);
                
                // Update weights
                w3 -= lr * d_output * h1;
                w4 -= lr * d_output * h2;
                b3 -= lr * d_output;
                
                w1 -= lr * d_h1 * x;
                w2 -= lr * d_h2 * x;
                b1 -= lr * d_h1;
                b2 -= lr * d_h2;
            }
            
            resultText += `Final weights after ${epochs} epochs:\n`;
            resultText += `w1=${w1.toFixed(4)}, w2=${w2.toFixed(4)}, w3=${w3.toFixed(4)}\n`;
            resultText += `w4=${w4.toFixed(4)}, w5=${w5.toFixed(4)}, w6=${w6.toFixed(4)}\n\n`;
            
            // Final prediction
            const h1 = 1 / (1 + Math.exp(-(w1 * x + b1)));
            const h2 = 1 / (1 + Math.exp(-(w2 * x + b2)));
            const finalOutput = 1 / (1 + Math.exp(-(w3 * h1 + w4 * h2 + b3)));
            
            resultText += `Final prediction: ${finalOutput.toFixed(4)}\n`;
            resultText += `Target: ${target}\n`;
            resultText += `Final loss: ${losses[losses.length - 1].toFixed(6)}`;
            
            backpropResult.textContent = resultText;
            backpropResult.style.display = 'block';
            
            // Update chart
            const epochLabels = Array.from({length: epochs}, (_, i) => i + 1);
            backpropChart.data.labels = epochLabels;
            backpropChart.data.datasets[0].data = losses;
            backpropChart.update();
        });
        
        // Initialize backpropagation
        runBackpropBtn.click();
        
        // Convolution demo
        const kernelSize = document.getElementById('kernel-size');
        const stride = document.getElementById('stride');
        const padding = document.getElementById('padding');
        const applyConvolutionBtn = document.getElementById('apply-convolution');
        const convolutionResult = document.getElementById('convolution-result');
        const convolutionCtx = document.getElementById('convolution-chart').getContext('2d');
        
        const convolutionChart = new Chart(convolutionCtx, {
            type: 'bar',
            data: {
                labels: [],
                datasets: [{
                    label: 'Feature Map',
                    data: [],
                    backgroundColor: 'rgba(54, 162, 235, 0.7)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Value'
                        }
                    }
                }
            }
        });
        
        applyConvolutionBtn.addEventListener('click', () => {
            const k = parseInt(kernelSize.value);
            const s = parseInt(stride.value);
            const p = padding.value === 'same';
            
            // Create a simple 5x5 input
            const input = [
                [1, 2, 3, 4, 5],
                [6, 7, 8, 9, 10],
                [11, 12, 13, 14, 15],
                [16, 17, 18, 19, 20],
                [21, 22, 23, 24, 25]
            ];
            
            // Create a simple kernel
            const kernel = [];
            for (let i = 0; i < k; i++) {
                kernel[i] = [];
                for (let j = 0; j < k; j++) {
                    kernel[i][j] = Math.random() * 2 - 1;  // Random values between -1 and 1
                }
            }
            
            // Calculate output size
            const inputSize = 5;
            let outputSize;
            
            if (p) {
                outputSize = Math.ceil(inputSize / s);
            } else {
                outputSize = Math.floor((inputSize - k) / s) + 1;
            }
            
            // Apply convolution
            const output = [];
            
            for (let i = 0; i < outputSize; i++) {
                output[i] = [];
                for (let j = 0; j < outputSize; j++) {
                    let sum = 0;
                    
                    for (let m = 0; m < k; m++) {
                        for (let n = 0; n < k; n++) {
                            const input_i = i * s + m;
                            const input_j = j * s + n;
                            
                            if (input_i >= 0 && input_i < inputSize && input_j >= 0 && input_j < inputSize) {
                                sum += input[input_i][input_j] * kernel[m][n];
                            }
                        }
                    }
                    
                    output[i][j] = sum;
                }
            }
            
            // Update result box
            let resultText = `Convolution Operation\n\n`;
            resultText += `Input (5x5):\n`;
            for (let i = 0; i < 5; i++) {
                resultText += input[i].join(' ') + '\n';
            }
            
            resultText += `\nKernel (${k}x${k}):\n`;
            for (let i = 0; i < k; i++) {
                resultText += kernel[i].map(x => x.toFixed(2)).join(' ') + '\n';
            }
            
            resultText += `\nStride: ${s}, Padding: ${padding}\n`;
            resultText += `Output size: ${outputSize}x${outputSize}\n\n`;
            resultText += `Output:\n`;
            for (let i = 0; i < outputSize; i++) {
                resultText += output[i].map(x => x.toFixed(2)).join(' ') + '\n';
            }
            
            convolutionResult.textContent = resultText;
            convolutionResult.style.display = 'block';
            
            // Update chart
            const labels = [];
            const data = [];
            
            for (let i = 0; i < outputSize; i++) {
                for (let j = 0; j < outputSize; j++) {
                    labels.push(`(${i},${j})`);
                    data.push(output[i][j]);
                }
            }
            
            convolutionChart.data.labels = labels;
            convolutionChart.data.datasets[0].data = data;
            convolutionChart.update();
        });
        
        // Initialize convolution
        applyConvolutionBtn.click();
        
        // Pooling demo
        const poolSize = document.getElementById('pool-size');
        const poolType = document.getElementById('pool-type');
        const poolStride = document.getElementById('pool-stride');
        const applyPoolingBtn = document.getElementById('apply-pooling');
        const poolingResult = document.getElementById('pooling-result');
        const poolingCtx = document.getElementById('pooling-chart').getContext('2d');
        
        const poolingChart = new Chart(poolingCtx, {
            type: 'bar',
            data: {
                labels: [],
                datasets: [{
                    label: 'Pooled Output',
                    data: [],
                    backgroundColor: 'rgba(255, 99, 132, 0.7)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Value'
                        }
                    }
                }
            }
        });
        
        applyPoolingBtn.addEventListener('click', () => {
            const p = parseInt(poolSize.value);
            const type = poolType.value;
            const s = parseInt(poolStride.value);
            
            // Create a simple 6x6 input
            const input = [];
            for (let i = 0; i < 6; i++) {
                input[i] = [];
                for (let j = 0; j < 6; j++) {
                    input[i][j] = Math.floor(Math.random() * 10);  // Random values 0-9
                }
            }
            
            // Calculate output size
            const inputSize = 6;
            const outputSize = Math.floor((inputSize - p) / s) + 1;
            
            // Apply pooling
            const output = [];
            
            for (let i = 0; i < outputSize; i++) {
                output[i] = [];
                for (let j = 0; j < outputSize; j++) {
                    const values = [];
                    
                    for (let m = 0; m < p; m++) {
                        for (let n = 0; n < p; n++) {
                            const input_i = i * s + m;
                            const input_j = j * s + n;
                            
                            if (input_i >= 0 && input_i < inputSize && input_j >= 0 && input_j < inputSize) {
                                values.push(input[input_i][input_j]);
                            }
                        }
                    }
                    
                    if (type === 'max') {
                        output[i][j] = Math.max(...values);
                    } else {  // avg
                        output[i][j] = values.reduce((a, b) => a + b, 0) / values.length;
                    }
                }
            }
            
            // Update result box
            let resultText = `${type === 'max' ? 'Max' : 'Average'} Pooling Operation\n\n`;
            resultText += `Input (6x6):\n`;
            for (let i = 0; i < 6; i++) {
                resultText += input[i].join(' ') + '\n';
            }
            
            resultText += `\nPool size: ${p}x${p}, Stride: ${s}\n`;
            resultText += `Output size: ${outputSize}x${outputSize}\n\n`;
            resultText += `Output:\n`;
            for (let i = 0; i < outputSize; i++) {
                resultText += output[i].map(x => x.toFixed(2)).join(' ') + '\n';
            }
            
            poolingResult.textContent = resultText;
            poolingResult.style.display = 'block';
            
            // Update chart
            const labels = [];
            const data = [];
            
            for (let i = 0; i < outputSize; i++) {
                for (let j = 0; j < outputSize; j++) {
                    labels.push(`(${i},${j})`);
                    data.push(output[i][j]);
                }
            }
            
            poolingChart.data.labels = labels;
            poolingChart.data.datasets[0].data = data;
            poolingChart.update();
        });
        
        // Initialize pooling
        applyPoolingBtn.click();
        
        // CNN model builder
        const dataset = document.getElementById('dataset');
        const convLayers = document.getElementById('conv-layers');
        const filters = document.getElementById('filters');
        const buildCnnBtn = document.getElementById('build-cnn');
        const cnnArchitecture = document.getElementById('cnn-architecture');
        
        buildCnnBtn.addEventListener('click', () => {
            const ds = dataset.value;
            const cl = parseInt(convLayers.value);
            const f = parseInt(filters.value);
            
            let code = `# CNN Architecture for ${ds.toUpperCase()}\n\n`;
            code += `import tensorflow as tf\n`;
            code += `from tensorflow import keras\n`;
            code += `from tensorflow.keras import layers, models\n\n`;
            
            code += `def build_${ds}_cnn():\n`;
            code += `    model = models.Sequential()\n\n`;
            
            if (ds === 'mnist') {
                code += `    # Input shape: 28x28x1 (grayscale)\n`;
                code += `    model.add(layers.Conv2D(${f}, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n`;
                code += `    model.add(layers.MaxPooling2D((2, 2)))\n\n`;
                
                for (let i = 1; i < cl; i++) {
                    code += `    model.add(layers.Conv2D(${f * (i + 1)}, (3, 3), activation='relu'))\n`;
                    code += `    model.add(layers.MaxPooling2D((2, 2)))\n\n`;
                }
                
                code += `    model.add(layers.Flatten())\n`;
                code += `    model.add(layers.Dense(64, activation='relu'))\n`;
                code += `    model.add(layers.Dense(10, activation='softmax'))\n\n`;
            } else if (ds === 'cifar10') {
                code += `    # Input shape: 32x32x3 (RGB)\n`;
                code += `    model.add(layers.Conv2D(${f}, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n`;
                code += `    model.add(layers.MaxPooling2D((2, 2)))\n\n`;
                
                for (let i = 1; i < cl; i++) {
                    code += `    model.add(layers.Conv2D(${f * (i + 1)}, (3, 3), activation='relu'))\n`;
                    code += `    model.add(layers.MaxPooling2D((2, 2)))\n\n`;
                }
                
                code += `    model.add(layers.Flatten())\n`;
                code += `    model.add(layers.Dense(64, activation='relu'))\n`;
                code += `    model.add(layers.Dense(10, activation='softmax'))\n\n`;
            }
            
            code += `    return model\n\n`;
            
            code += `# Build and compile the model\n`;
            code += `model = build_${ds}_cnn()\n`;
            code += `model.compile(optimizer='adam',\n`;
            code += `              loss='sparse_categorical_crossentropy',\n`;
            code += `              metrics=['accuracy'])\n\n`;
            
            code += `# Display model architecture\n`;
            code += `model.summary()\n`;
            
            cnnArchitecture.textContent = code;
            cnnArchitecture.style.display = 'block';
        });
        
        // Initialize CNN model builder
        buildCnnBtn.click();
        
        // RNN/LSTM visualization
        function visualizeRNN(type, sequenceLength, hiddenSize, numLayers) {
            const visualizationId = `${type}-visualization`;
            const resultId = `${type}-result`;
            const visualization = document.getElementById(visualizationId);
            const result = document.getElementById(resultId);
            
            visualization.innerHTML = '';
            
            // Create input layer
            const inputLayer = document.createElement('div');
            inputLayer.className = 'layer';
            inputLayer.innerHTML = '<h4>Input</h4>';
            
            for (let t = 0; t < sequenceLength; t++) {
                const neuron = document.createElement('div');
                neuron.className = 'neuron';
                neuron.textContent = `x${t}`;
                inputLayer.appendChild(neuron);
            }
            
            visualization.appendChild(inputLayer);
            
            // Create RNN/LSTM layers
            for (let l = 0; l < numLayers; l++) {
                // Create connection
                const connection = document.createElement('div');
                connection.className = 'connection';
                
                const weightLabel = document.createElement('div');
                weightLabel.className = 'connection-weight';
                weightLabel.textContent = 'W';
                connection.appendChild(weightLabel);
                
                visualization.appendChild(connection);
                
                // Create RNN/LSTM layer
                const layer = document.createElement('div');
                layer.className = 'layer';
                layer.innerHTML = `<h4>${type.toUpperCase()} Layer ${l+1}</h4>`;
                
                for (let t = 0; t < sequenceLength; t++) {
                    const neuron = document.createElement('div');
                    neuron.className = 'neuron';
                    neuron.textContent = `h${l+1},${t}`;
                    layer.appendChild(neuron);
                }
                
                visualization.appendChild(layer);
            }
            
            // Create connection to output
            const connection = document.createElement('div');
            connection.className = 'connection';
            
            const weightLabel = document.createElement('div');
            weightLabel.className = 'connection-weight';
            weightLabel.textContent = 'W';
            connection.appendChild(weightLabel);
            
            visualization.appendChild(connection);
            
            // Create output layer
            const outputLayer = document.createElement('div');
            outputLayer.className = 'layer';
            outputLayer.innerHTML = '<h4>Output</h4>';
            
            const neuron = document.createElement('div');
            neuron.className = 'neuron';
            neuron.textContent = 'y';
            outputLayer.appendChild(neuron);
            
            visualization.appendChild(outputLayer);
            
            // Update result box
            let code = `# ${type.toUpperCase()} Implementation\n\n`;
            code += `import tensorflow as tf\n`;
            code += `from tensorflow import keras\n`;
            code += `from tensorflow.keras import layers, models\n\n`;
            
            code += `def build_${type}_model(sequence_length, hidden_size, num_layers):\n`;
            code += `    model = models.Sequential()\n\n`;
            
            if (type === 'rnn') {
                code += `    # RNN layers\n`;
                code += `    for i in range(num_layers):\n`;
                code += `        if i == 0:\n`;
                code += `            model.add(layers.SimpleRNN(hidden_size, return_sequences=(i < num_layers - 1), input_shape=(sequence_length, 1)))\n`;
                code += `        else:\n`;
                code += `            model.add(layers.SimpleRNN(hidden_size, return_sequences=(i < num_layers - 1)))\n`;
                code += `\n`;
            } else if (type === 'lstm') {
                code += `    # LSTM layers\n`;
                code += `    for i in range(num_layers):\n`;
                code += `        if i == 0:\n`;
                code += `            model.add(layers.LSTM(hidden_size, return_sequences=(i < num_layers - 1), input_shape=(sequence_length, 1)))\n`;
                code += `        else:\n`;
                code += `            model.add(layers.LSTM(hidden_size, return_sequences=(i < num_layers - 1)))\n`;
                code += `\n`;
            } else if (type === 'gru') {
                code += `    # GRU layers\n`;
                code += `    for i in range(num_layers):\n`;
                code += `        if i == 0:\n`;
                code += `            model.add(layers.GRU(hidden_size, return_sequences=(i < num_layers - 1), input_shape=(sequence_length, 1)))\n`;
                code += `        else:\n`;
                code += `            model.add(layers.GRU(hidden_size, return_sequences=(i < num_layers - 1)))\n`;
                code += `\n`;
            }
            
            code += `    # Output layer\n`;
            code += `    model.add(layers.Dense(1))\n\n`;
            code += `    return model\n\n`;
            
            code += `# Build the model\n`;
            code += `model = build_${type}_model(${sequenceLength}, ${hiddenSize}, ${numLayers})\n\n`;
            code += `# Compile the model\n`;
            code += `model.compile(optimizer='adam', loss='mse')\n\n`;
            code += `# Display model architecture\n`;
            code += `model.summary()\n`;
            
            result.textContent = code;
            result.style.display = 'block';
        }
        
        // RNN visualization
        const rnnSequence = document.getElementById('rnn-sequence');
        const rnnHidden = document.getElementById('rnn-hidden');
        const rnnLayers = document.getElementById('rnn-layers');
        const visualizeRnnBtn = document.getElementById('visualize-rnn');
        
        visualizeRnnBtn.addEventListener('click', () => {
            const seq = parseInt(rnnSequence.value);
            const hidden = parseInt(rnnHidden.value);
            const layers = parseInt(rnnLayers.value);
            
            visualizeRNN('rnn', seq, hidden, layers);
        });
        
        // LSTM visualization
        const lstmSequence = document.getElementById('lstm-sequence');
        const lstmHidden = document.getElementById('lstm-hidden');
        const lstmLayers = document.getElementById('lstm-layers');
        const visualizeLstmBtn = document.getElementById('visualize-lstm');
        
        visualizeLstmBtn.addEventListener('click', () => {
            const seq = parseInt(lstmSequence.value);
            const hidden = parseInt(lstmHidden.value);
            const layers = parseInt(lstmLayers.value);
            
            visualizeRNN('lstm', seq, hidden, layers);
        });
        
        // GRU visualization
        const gruSequence = document.getElementById('gru-sequence');
        const gruHidden = document.getElementById('gru-hidden');
        const gruLayers = document.getElementById('gru-layers');
        const visualizeGruBtn = document.getElementById('visualize-gru');
        
        visualizeGruBtn.addEventListener('click', () => {
            const seq = parseInt(gruSequence.value);
            const hidden = parseInt(gruHidden.value);
            const layers = parseInt(gruLayers.value);
            
            visualizeRNN('gru', seq, hidden, layers);
        });
        
        // Initialize RNN/LSTM visualizations
        visualizeRnnBtn.click();
        visualizeLstmBtn.click();
        visualizeGruBtn.click();
        
        // Sequence model builder
        const taskType = document.getElementById('task-type');
        const modelType = document.getElementById('model-type');
        const embeddingDim = document.getElementById('embedding-dim');
        const buildSequenceModelBtn = document.getElementById('build-sequence-model');
        const sequenceModelCode = document.getElementById('sequence-model-code');
        
        buildSequenceModelBtn.addEventListener('click', () => {
            const task = taskType.value;
            const model = modelType.value;
            const embedDim = parseInt(embeddingDim.value);
            
            let code = `# ${model.toUpperCase()} Model for ${task.replace('-', ' ').toUpperCase()}\n\n`;
            code += `import tensorflow as tf\n`;
            code += `from tensorflow import keras\n`;
            code += `from tensorflow.keras import layers, models, preprocessing\n\n`;
            
            if (task === 'text-classification' || task === 'sentiment-analysis') {
                code += `# Text preprocessing parameters\n`;
                code += `max_features = 10000  # Number of words to consider as features\n`;
                code += `maxlen = 200  # Maximum sequence length\n`;
                code += `embedding_dim = ${embedDim}\n\n`;
                
                code += `# Build the model\n`;
                code += `def build_${model.replace('-', '_')}_model():\n`;
                code += `    model = models.Sequential()\n`;
                code += `    \n`;
                code += `    # Embedding layer\n`;
                code += `    model.add(layers.Embedding(max_features, embedding_dim, input_length=maxlen))\n`;
                code += `    \n`;
                
                if (model === 'rnn') {
                    code += `    # RNN layer\n`;
                    code += `    model.add(layers.SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n`;
                } else if (model === 'lstm') {
                    code += `    # LSTM layer\n`;
                    code += `    model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n`;
                } else if (model === 'gru') {
                    code += `    # GRU layer\n`;
                    code += `    model.add(layers.GRU(128, dropout=0.2, recurrent_dropout=0.2))\n`;
                } else if (model === 'bidirectional') {
                    code += `    # Bidirectional LSTM layer\n`;
                    code += `    model.add(layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n`;
                }
                
                code += `    \n`;
                code += `    # Dense layers\n`;
                code += `    model.add(layers.Dense(64, activation='relu'))\n`;
                code += `    model.add(layers.Dropout(0.5))\n`;
                code += `    \n`;
                code += `    # Output layer\n`;
                if (task === 'sentiment-analysis') {
                    code += `    model.add(layers.Dense(1, activation='sigmoid'))\n`;
                    code += `    \n`;
                    code += `    # Compile the model\n`;
                    code += `    model.compile(optimizer='adam',\n`;
                    code += `                  loss='binary_crossentropy',\n`;
                    code += `                  metrics=['accuracy'])\n`;
                } else {
                    code += `    model.add(layers.Dense(num_classes, activation='softmax'))\n`;
                    code += `    \n`;
                    code += `    # Compile the model\n`;
                    code += `    model.compile(optimizer='adam',\n`;
                    code += `                  loss='sparse_categorical_crossentropy',\n`;
                    code += `                  metrics=['accuracy'])\n`;
                }
                
                code += `    \n`;
                code += `    return model\n\n`;
                
                code += `# Build and display the model\n`;
                code += `model = build_${model.replace('-', '_')}_model()\n`;
                code += `model.summary()\n`;
                
            } else if (task === 'time-series') {
                code += `# Time series parameters\n`;
                code += `n_steps = 10  # Number of time steps\n`;
                code += `n_features = 1  # Number of features\n`;
                code += `embedding_dim = ${embedDim}\n\n`;
                
                code += `# Build the model\n`;
                code += `def build_${model.replace('-', '_')}_model():\n`;
                code += `    model = models.Sequential()\n`;
                code += `    \n`;
                
                if (model === 'rnn') {
                    code += `    # RNN layer\n`;
                    code += `    model.add(layers.SimpleRNN(50, activation='relu', input_shape=(n_steps, n_features)))\n`;
                } else if (model === 'lstm') {
                    code += `    # LSTM layer\n`;
                    code += `    model.add(layers.LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n`;
                } else if (model === 'gru') {
                    code += `    # GRU layer\n`;
                    code += `    model.add(layers.GRU(50, activation='relu', input_shape=(n_steps, n_features)))\n`;
                } else if (model === 'bidirectional') {
                    code += `    # Bidirectional LSTM layer\n`;
                    code += `    model.add(layers.Bidirectional(layers.LSTM(50, activation='relu', input_shape=(n_steps, n_features))))\n`;
                }
                
                code += `    \n`;
                code += `    # Dense layers\n`;
                code += `    model.add(layers.Dense(25, activation='relu'))\n`;
                code += `    \n`;
                code += `    # Output layer\n`;
                code += `    model.add(layers.Dense(1))\n`;
                code += `    \n`;
                code += `    # Compile the model\n`;
                code += `    model.compile(optimizer='adam', loss='mse')\n`;
                code += `    \n`;
                code += `    return model\n\n`;
                
                code += `# Build and display the model\n`;
                code += `model = build_${model.replace('-', '_')}_model()\n`;
                code += `model.summary()\n`;
            }
            
            sequenceModelCode.textContent = code;
            sequenceModelCode.style.display = 'block';
        });
        
        // Initialize sequence model builder
        buildSequenceModelBtn.click();
    </script>
</body>
</html>